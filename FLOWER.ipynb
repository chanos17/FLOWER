{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r4TEBe9UVB-jEbLoqt5fXHj_q_UrKJNJ",
      "authorship_tag": "ABX9TyMlXWXDafrVrDCGnkrvZvet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanos17/FLOWER/blob/main/FLOWER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mr9fCnu8UJdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class CellDataset(Dataset):\n",
        "    def __init__(self, data_folder, transform=None):\n",
        "        self.data_folder = data_folder\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(data_folder) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.data_folder, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # 이미지 이름에서 레이블 추출\n",
        "        label = int(re.search(r'box_(\\d+)', img_name).group(1))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터셋 경로 및 변환 정의\n",
        "data_folder = '/content/Data_Folder'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "dataset = CellDataset(data_folder, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "KcY_QSQYUUmh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CellCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CellCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(64*14*14, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_ie_Ioi2UU6d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CellCNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "OlwPYuVFUVNF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        # 레이블을 float 형태로 변환\n",
        "        labels = labels.float()\n",
        "\n",
        "        # 옵티마이저 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 모델 예측\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        # 역전파 및 옵티마이저 스텝\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnmDI3iVUVRq",
        "outputId": "92bbd3b8-8f07-4568-b96a-050882121d2a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 112.0326\n",
            "Epoch [2/50], Loss: 100.1496\n",
            "Epoch [3/50], Loss: 80.6561\n",
            "Epoch [4/50], Loss: 52.8079\n",
            "Epoch [5/50], Loss: 22.6792\n",
            "Epoch [6/50], Loss: 16.0271\n",
            "Epoch [7/50], Loss: 40.8453\n",
            "Epoch [8/50], Loss: 29.3941\n",
            "Epoch [9/50], Loss: 15.4197\n",
            "Epoch [10/50], Loss: 12.8045\n",
            "Epoch [11/50], Loss: 16.6104\n",
            "Epoch [12/50], Loss: 20.7613\n",
            "Epoch [13/50], Loss: 22.6982\n",
            "Epoch [14/50], Loss: 22.1257\n",
            "Epoch [15/50], Loss: 19.6666\n",
            "Epoch [16/50], Loss: 16.3446\n",
            "Epoch [17/50], Loss: 13.5256\n",
            "Epoch [18/50], Loss: 12.5835\n",
            "Epoch [19/50], Loss: 13.9806\n",
            "Epoch [20/50], Loss: 16.3081\n",
            "Epoch [21/50], Loss: 17.1820\n",
            "Epoch [22/50], Loss: 15.8733\n",
            "Epoch [23/50], Loss: 13.8251\n",
            "Epoch [24/50], Loss: 12.6501\n",
            "Epoch [25/50], Loss: 12.7622\n",
            "Epoch [26/50], Loss: 13.6087\n",
            "Epoch [27/50], Loss: 14.4317\n",
            "Epoch [28/50], Loss: 14.7383\n",
            "Epoch [29/50], Loss: 14.4107\n",
            "Epoch [30/50], Loss: 13.6550\n",
            "Epoch [31/50], Loss: 12.8868\n",
            "Epoch [32/50], Loss: 12.5450\n",
            "Epoch [33/50], Loss: 12.8185\n",
            "Epoch [34/50], Loss: 13.3437\n",
            "Epoch [35/50], Loss: 13.5630\n",
            "Epoch [36/50], Loss: 13.3140\n",
            "Epoch [37/50], Loss: 12.8632\n",
            "Epoch [38/50], Loss: 12.5676\n",
            "Epoch [39/50], Loss: 12.5641\n",
            "Epoch [40/50], Loss: 12.7489\n",
            "Epoch [41/50], Loss: 12.9315\n",
            "Epoch [42/50], Loss: 12.9740\n",
            "Epoch [43/50], Loss: 12.8587\n",
            "Epoch [44/50], Loss: 12.6715\n",
            "Epoch [45/50], Loss: 12.5366\n",
            "Epoch [46/50], Loss: 12.5317\n",
            "Epoch [47/50], Loss: 12.6306\n",
            "Epoch [48/50], Loss: 12.7260\n",
            "Epoch [49/50], Loss: 12.7258\n",
            "Epoch [50/50], Loss: 12.6343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_cell_count(model, image_path):\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        predicted_count = output.item()\n",
        "\n",
        "    return predicted_count\n",
        "\n",
        "# 예측하기\n",
        "image_path = '/content/Data_Folder/Testdata/MS1096_969OE_29_male_left_2_A.png'\n",
        "predicted_count = predict_cell_count(model, image_path)\n",
        "print(f'Predicted Cell Count: {predicted_count:.0f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO66Mt_IUVa6",
        "outputId": "79e83c5c-7ec9-406f-f408-d48909bdb72f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Cell Count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_cell_count(model, image_path):\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        predicted_count = output.item()\n",
        "\n",
        "    return predicted_count\n",
        "\n",
        "# 예측하기\n",
        "image_path = '/content/Data_Folder/Testdata/MS1096_969OE_29_male_left_2_B.png'\n",
        "predicted_count = predict_cell_count(model, image_path)\n",
        "print(f'Predicted Cell Count: {predicted_count:.0f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHKOjGBZUV04",
        "outputId": "55c7b835-b4e5-45df-fe26-088c012da1c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Cell Count: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def detect_red_box(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # 빨간색 범위를 HSV 색공간에서 설정\n",
        "    lower_red = np.array([0, 100, 100])\n",
        "    upper_red = np.array([10, 255, 255])\n",
        "\n",
        "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # 가장 큰 컨투어(빨간 박스) 선택\n",
        "    if contours:\n",
        "        red_box = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(red_box)\n",
        "        return x, y, w, h\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 빨간 박스를 인식하여 자르기\n",
        "image_path = '/content/Data_Folder/MS1096_w1118_29_male_left_1_label1box_12.png'\n",
        "red_box_coords = detect_red_box(image_path)\n",
        "\n",
        "if red_box_coords:\n",
        "    x, y, w, h = red_box_coords\n",
        "    image = Image.open(image_path)\n",
        "    red_box_image = image.crop((x, y, x+w, y+h))\n",
        "    red_box_image.show()\n"
      ],
      "metadata": {
        "id": "OXuJiocFUWMc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_cell_count_in_box(model, red_box_image):\n",
        "    model.eval()\n",
        "\n",
        "    red_box_image = transform(red_box_image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(red_box_image)\n",
        "        predicted_count = output.item()\n",
        "\n",
        "    return predicted_count\n",
        "\n",
        "# 자른 빨간 박스에서 세포 수 예측\n",
        "predicted_count = predict_cell_count_in_box(model, red_box_image)\n",
        "print(f'Predicted Cell Count in Red Box: {predicted_count:.0f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSGnV_QMUWQl",
        "outputId": "c7623128-d8dc-4017-c9b4-6e29c6f1754a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Cell Count in Red Box: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빨간 박스를 인식하여 자르기\n",
        "image_path = '/content/Data_Folder/Testdata/MS1096_969OE_29_male_left_2_B.png'\n",
        "red_box_coords = detect_red_box(image_path)\n",
        "\n",
        "if red_box_coords:\n",
        "    x, y, w, h = red_box_coords\n",
        "    image = Image.open(image_path)\n",
        "    red_box_image = image.crop((x, y, x+w, y+h))\n",
        "    red_box_image.show()\n",
        "\n",
        "# 자른 빨간 박스에서 세포 수 예측\n",
        "predicted_count = predict_cell_count_in_box(model, red_box_image)\n",
        "print(f'Predicted Cell Count in Red Box: {predicted_count:.0f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVF2e3xIUWUg",
        "outputId": "79b81750-bbdf-4c5d-c0f5-ddd0ac70eea2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Cell Count in Red Box: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빨간 박스를 인식하여 자르기\n",
        "image_path = '/content/Data_Folder/Testdata/MS1096_969OE_29_male_left_2_A.png'\n",
        "red_box_coords = detect_red_box(image_path)\n",
        "\n",
        "if red_box_coords:\n",
        "    x, y, w, h = red_box_coords\n",
        "    image = Image.open(image_path)\n",
        "    red_box_image = image.crop((x, y, x+w, y+h))\n",
        "    red_box_image.show()\n",
        "\n",
        "# 자른 빨간 박스에서 세포 수 예측\n",
        "predicted_count = predict_cell_count_in_box(model, red_box_image)\n",
        "print(f'Predicted Cell Count in Red Box: {predicted_count:.0f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR52dCCeUVHf",
        "outputId": "e6ca80c1-0937-4b99-cc96-a8f91fbafec1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Cell Count in Red Box: 10\n"
          ]
        }
      ]
    }
  ]
}